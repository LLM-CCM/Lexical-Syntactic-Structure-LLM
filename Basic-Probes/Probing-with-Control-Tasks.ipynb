{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "084ea5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "75a8d767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==2.1 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (2.1.0)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from transformers==2.1) (0.1.98)\n",
      "Requirement already satisfied: requests in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from transformers==2.1) (2.28.1)\n",
      "Requirement already satisfied: boto3 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from transformers==2.1) (1.24.28)\n",
      "Requirement already satisfied: regex in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from transformers==2.1) (2022.7.9)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from transformers==2.1) (0.0.53)\n",
      "Requirement already satisfied: numpy in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from transformers==2.1) (1.21.5)\n",
      "Requirement already satisfied: tqdm in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from transformers==2.1) (4.64.1)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from boto3->transformers==2.1) (0.10.0)\n",
      "Requirement already satisfied: botocore<1.28.0,>=1.27.28 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from boto3->transformers==2.1) (1.27.28)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from boto3->transformers==2.1) (0.6.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from requests->transformers==2.1) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from requests->transformers==2.1) (2022.9.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from requests->transformers==2.1) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from requests->transformers==2.1) (1.26.11)\n",
      "Requirement already satisfied: six in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from sacremoses->transformers==2.1) (1.16.0)\n",
      "Requirement already satisfied: click in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from sacremoses->transformers==2.1) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from sacremoses->transformers==2.1) (1.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from tqdm->transformers==2.1) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from botocore<1.28.0,>=1.27.28->boto3->transformers==2.1) (2.8.2)\n",
      "Requirement already satisfied: spacy in c:\\users\\harsh\\anaconda3\\lib\\site-packages (3.5.2)\n",
      "Requirement already satisfied: ftfy==4.4.3 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (4.4.3)\n",
      "Requirement already satisfied: html5lib in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from ftfy==4.4.3) (1.1)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from ftfy==4.4.3) (0.2.5)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from spacy) (5.2.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from spacy) (1.21.5)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from spacy) (8.1.9)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from spacy) (2.28.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from spacy) (1.10.7)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from spacy) (1.0.4)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from spacy) (0.7.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from spacy) (2.4.6)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from spacy) (4.64.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: setuptools in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from spacy) (63.4.1)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from spacy) (1.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy) (3.0.9)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.3.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.9.14)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.0.4)\n",
      "Requirement already satisfied: six>=1.9 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from html5lib->ftfy==4.4.3) (1.16.0)\n",
      "Requirement already satisfied: webencodings in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from html5lib->ftfy==4.4.3) (0.5.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.0.1)\n",
      "Collecting en-core-web-sm==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 12.8/12.8 MB 8.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.6.0,>=3.5.0 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from en-core-web-sm==3.5.0) (3.5.2)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.9)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.7)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.11.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (21.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (63.4.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.21.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.64.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (5.2.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.28.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.9)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2022.9.14)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.1)\n",
      "\u001b[38;5;3m[!] As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use\n",
      "the full pipeline package name 'en_core_web_sm' instead.\u001b[0m\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "#!git clone https://github.com/SIDN-IAP/global-model-repr.git datasource\n",
    "!pip install transformers==2.1\n",
    "!pip install spacy ftfy==4.4.3\n",
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e70338f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "sys.path.append('./datasource')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cpu')\n",
    "else:\n",
    "    print(\"Change runtime type to include a GPU.\")  \n",
    "    device = torch.device('cpu')\n",
    "print(\"device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "172c87ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "from transformers import GPT2Tokenizer, GPT2Model, BertTokenizer, BertModel\n",
    "\n",
    "DATA_DIR = \"data\"\n",
    "UD_EN_PREF = \"en-ud-\"\n",
    "\n",
    "def get_model_and_tokenizer(model_name, device, random_weights=False):\n",
    "\n",
    "    model_name = model_name\n",
    "\n",
    "    if model_name.startswith('gpt2'):\n",
    "        model = GPT2Model.from_pretrained(model_name, output_hidden_states=True).to(device)\n",
    "        tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "        sep = 'Ä '\n",
    "        sizes = {\"gpt2\": 768, \"gpt2-medium\": 1024, \"gpt2-large\": 1280, \"gpt2-xl\": 1600}\n",
    "        emb_dim = sizes[model_name]\n",
    "    elif model_name.startswith('bert'):\n",
    "        model = BertModel.from_pretrained(model_name, output_hidden_states=True).to(device)\n",
    "        tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        sep = '##'\n",
    "        emb_dim = 1024 if \"large\" in model_name else 768\n",
    "    else:\n",
    "        print('Unrecognized model name:', model_name)\n",
    "        sys.exit()\n",
    "\n",
    "    if random_weights:\n",
    "        print('Randomizing weights')\n",
    "        model.init_weights()\n",
    "\n",
    "    return model, tokenizer, sep, emb_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "95149953",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_repr(sentence, model, tokenizer, sep, model_name, device):\n",
    "    \"\"\"\n",
    "    Get representations for one sentence\n",
    "    \"\"\"\n",
    "\n",
    "    with torch.no_grad():\n",
    "        ids = tokenizer.encode(sentence)\n",
    "        input_ids = torch.tensor([ids]).to(device)\n",
    "        # Hugging Face format: list of torch.FloatTensor of shape (batch_size, sequence_length, hidden_size) (hidden_states at output of each layer plus initial embedding outputs)\n",
    "        all_hidden_states = model(input_ids)[-1]\n",
    "        # convert to format required for contexteval: numpy array of shape (num_layers, sequence_length, representation_dim)\n",
    "        all_hidden_states = [hidden_states[0].cpu().numpy() for hidden_states in all_hidden_states]\n",
    "        all_hidden_states = np.array(all_hidden_states)\n",
    "\n",
    "    #For each word, take the representation of its last sub-word\n",
    "    segmented_tokens = tokenizer.convert_ids_to_tokens(ids)\n",
    "    assert len(segmented_tokens) == all_hidden_states.shape[1], 'incompatible tokens and states'\n",
    "    mask = np.full(len(segmented_tokens), False)\n",
    "\n",
    "    if model_name.startswith('gpt2'):\n",
    "        # if next token is a new word, take current token's representation\n",
    "        #print(segmented_tokens)\n",
    "        for i in range(len(segmented_tokens)-1):\n",
    "            if segmented_tokens[i+1].startswith(sep):\n",
    "                #print(i)\n",
    "                mask[i] = True\n",
    "        # always take the last token representation for the last word\n",
    "        mask[-1] = True\n",
    "    # example: ['jim</w>', 'henson</w>', 'was</w>', 'a</w>', 'pup', 'pe', 'teer</w>']\n",
    "    elif model_name.startswith('bert'):\n",
    "        # if next token is not a continuation, take current token's representation\n",
    "        for i in range(len(segmented_tokens)-1):\n",
    "            if not segmented_tokens[i+1].startswith(sep):\n",
    "                mask[i] = True\n",
    "        mask[-1] = True\n",
    "    else:\n",
    "        print('Unrecognized model name:', model_name)\n",
    "        sys.exit()\n",
    "\n",
    "    all_hidden_states = all_hidden_states[:, mask]\n",
    "    # all_hidden_states = torch.tensor(all_hidden_states).to(device)\n",
    "\n",
    "    return all_hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5dfc3476",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_data(probing_dir, frac=1.0, device='cpu'):\n",
    "\n",
    "    return get_data(\"pos\", probing_dir=probing_dir, frac=frac, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a95a6cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(data_type, probing_dir, data_pref=UD_EN_PREF, frac=1.0, device='cpu'):\n",
    "\n",
    "    with open(os.path.join(probing_dir, DATA_DIR, data_pref + \"train.txt\"), encoding=\"utf8\") as f:\n",
    "        train_sentences = [line.strip().split() for line in f.readlines()]\n",
    "    with open(os.path.join(probing_dir, DATA_DIR, data_pref + \"test.txt\"), encoding=\"utf8\") as f:\n",
    "        test_sentences = [line.strip().split() for line in f.readlines()]\n",
    "    with open(os.path.join(probing_dir, DATA_DIR, data_pref + \"dev.txt\"), encoding=\"utf8\") as f:\n",
    "        dev_sentences = [line.strip().split() for line in f.readlines()]\n",
    "\n",
    "    with open(os.path.join(probing_dir, DATA_DIR, data_pref + \"train.\" + data_type), encoding=\"utf8\") as f:\n",
    "        train_labels = [line.strip().split() for line in f.readlines()]\n",
    "    with open(os.path.join(probing_dir, DATA_DIR, data_pref + \"test.\" + data_type), encoding=\"utf8\") as f:\n",
    "        test_labels = [line.strip().split() for line in f.readlines()]\n",
    "    with open(os.path.join(probing_dir, DATA_DIR, data_pref + \"dev.\" + data_type), encoding=\"utf8\") as f:\n",
    "        dev_labels = [line.strip().split() for line in f.readlines()]\n",
    "\n",
    "    # take a fraction of the data\n",
    "    train_sentences = train_sentences[:round(len(train_sentences)*frac)]\n",
    "    test_sentences = train_sentences[:round(len(test_sentences)*frac)]\n",
    "    dev_sentences = train_sentences[:round(len(dev_sentences)*frac)]\n",
    "    train_labels = train_labels[:round(len(train_labels)*frac)]\n",
    "    test_labels = train_labels[:round(len(test_labels)*frac)]\n",
    "    dev_labels = train_labels[:round(len(dev_labels)*frac)]\n",
    "\n",
    "    unique_labels = list(set.union(*[set(l) for l in train_labels + test_labels + dev_labels]))\n",
    "    label2index = dict()\n",
    "    for label in unique_labels:\n",
    "        label2index[label] = label2index.get(label, len(label2index))\n",
    "\n",
    "    train_labels = [[label2index[l] for l in labels] for labels in train_labels]\n",
    "    test_labels = [[label2index[l] for l in labels] for labels in test_labels]\n",
    "    dev_labels = [[label2index[l] for l in labels] for labels in dev_labels]\n",
    "\n",
    "\n",
    "    return train_sentences, train_labels, test_sentences, test_labels, dev_sentences, dev_labels, label2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4882727f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training sentences: 1254 Test sentences: 208\n",
      "Unique labels: 17\n"
     ]
    }
   ],
   "source": [
    "train_sentences, train_labels, test_sentences, test_labels, _, _, label2index = get_pos_data(\"./datasource/probing/\", frac=0.1)\n",
    "# train_sentences, train_labels, test_sentences, test_labels, _, _, label2index = get_pos_data(\"../probing\", frac=0.1)\n",
    "num_labels = len(label2index)\n",
    "print(\"Training sentences:\", len(train_sentences), \"Test sentences:\", len(test_sentences))\n",
    "print(\"Unique labels:\", num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "01d18281",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Classifier, self).__init__()\n",
    "        \n",
    "        self.linear = torch.nn.Linear(input_dim, output_dim)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        output = self.linear(input)\n",
    "        return output\n",
    "    \n",
    "\n",
    "class NonlinearClassifier(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(NonlinearClassifier, self).__init__()\n",
    "        \n",
    "        self.input2hidden = torch.nn.Linear(input_dim, input_dim)\n",
    "        self.hidden2output = torch.nn.Linear(input_dim, output_dim)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        \n",
    "    def forward(self, input):\n",
    "        hidden = self.relu(self.input2hidden(input))\n",
    "        output = self.hidden2output(hidden)\n",
    "        return output\n",
    "    \n",
    "    \n",
    "def build_classifier(emb_dim, num_labels, device='cpu'):\n",
    "\n",
    "    classifier = Classifier(emb_dim, num_labels).to(device)\n",
    "    criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "    optimizer = torch.optim.Adam(classifier.parameters())\n",
    "\n",
    "    return classifier, criterion, optimizer\n",
    "\n",
    "\n",
    "def build_nonlinear_classifier(emb_dim, num_labels, device='cpu'):\n",
    "\n",
    "    classifier = NonlinearClassifier(emb_dim, num_labels).to(device)\n",
    "    criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "    optimizer = torch.optim.Adam(classifier.parameters())\n",
    "\n",
    "    return classifier, criterion, optimizer\n",
    "\n",
    "\n",
    "model_name = 'bert-base-cased'\n",
    "# get model and tokenizer from Transformers\n",
    "model, tokenizer, sep, emb_dim = get_model_and_tokenizer(model_name, device)\n",
    "# build classifier\n",
    "classifier, criterion, optimizer = build_classifier(emb_dim, num_labels, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "561873fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "967323f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier(\n",
      "  (linear): Linear(in_features=768, out_features=17, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4b72b994",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs, train_representations, train_labels, \n",
    "          model, tokenizer, sep, model_name, device, \n",
    "          classifier, criterion, optimizer, batch_size=32):\n",
    "    \n",
    "    num_total = train_representations.shape[0] \n",
    "    for i in range(num_epochs):\n",
    "        total_loss = 0.\n",
    "        num_correct = 0.\n",
    "        for batch in range(0, num_total, batch_size):\n",
    "            batch_repr = train_representations[batch: batch+batch_size]\n",
    "            batch_labels = train_labels[batch: batch+batch_size]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            out = classifier(batch_repr)\n",
    "            pred = out.max(1)[1]\n",
    "            num_correct += pred.long().eq(batch_labels.long()).cpu().sum().item()\n",
    "            loss = criterion(out, batch_labels.type(torch.LongTensor))\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "#         print('Training epoch: {}, loss: {}, accuracy: {}'.format(i, total_loss/num_total, num_correct/num_total))\n",
    "    return total_loss/num_total, num_correct/num_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "12b28e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(test_representations, test_labels, \n",
    "             model, tokenizer, sep, model_name, device, \n",
    "             classifier, criterion, batch_size=32):\n",
    "    \n",
    "    num_correct = 0.\n",
    "    num_total = test_representations.shape[0]\n",
    "    total_loss = 0.\n",
    "    with torch.no_grad():\n",
    "        for batch in range(0, num_total, batch_size):\n",
    "            batch_repr = test_representations[batch: batch+batch_size]\n",
    "            batch_labels = test_labels[batch: batch+batch_size]\n",
    "            \n",
    "            out = classifier(batch_repr)\n",
    "            pred = out.max(1)[1]\n",
    "            num_correct += pred.long().eq(batch_labels.long()).cpu().sum().item()\n",
    "            total_loss += criterion(out, batch_labels.type(torch.LongTensor))\n",
    "\n",
    "#     print('Testing loss: {}, accuracy: {}'.format(total_loss/num_total, num_correct/num_total))\n",
    "    return total_loss/num_total, num_correct/num_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f18661bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3f944e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top-level list: sentences, second-level lists: layers, third-level tensors of num_words x representation_dim\n",
    "train_sentence_representations = [get_sentence_repr(sentence, model, tokenizer, sep, model_name, device) \n",
    "                                  for sentence in train_sentences]\n",
    "test_sentence_representations = [get_sentence_repr(sentence, model, tokenizer, sep, model_name, device) \n",
    "                                  for sentence in test_sentences]\n",
    "\n",
    "# top-level list: layers, second-level lists: sentences\n",
    "train_sentence_representations = [list(l) for l in zip(*train_sentence_representations)]\n",
    "test_sentence_representations = [list(l) for l in zip(*test_sentence_representations)]                           \n",
    "\n",
    "# concatenate all word represenations\n",
    "train_representations_all = [torch.tensor(np.concatenate(train_layer_representations, 0)).to(device) for train_layer_representations in train_sentence_representations]\n",
    "test_representations_all = [torch.tensor(np.concatenate(test_layer_representations, 0)).to(device) for test_layer_representations in test_sentence_representations]\n",
    "# concatenate all labels\n",
    "train_labels_all = torch.tensor(np.concatenate(train_labels, 0)).to(device)\n",
    "test_labels_all = torch.tensor(np.concatenate(test_labels, 0)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4e0aff64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1254\n"
     ]
    }
   ],
   "source": [
    "print(len(train_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0dc30ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.9266106027488609, Test accuracy: 0.9172932330827067\n"
     ]
    }
   ],
   "source": [
    "# Take final layer representations\n",
    "train_representations = train_representations_all[-1]\n",
    "test_representations = test_representations_all[-1]\n",
    "\n",
    "# train\n",
    "train_loss, train_accuracy = train(10, train_representations, train_labels_all, \n",
    "          model, tokenizer, sep, model_name, device, \n",
    "          classifier, criterion, optimizer)\n",
    "# test\n",
    "test_loss, test_accuracy = evaluate(test_representations, test_labels_all, \n",
    "         model, tokenizer, sep, model_name, device, \n",
    "         classifier, criterion)\n",
    "print(\"Train accuracy: {}, Test accuracy: {}\".format(train_accuracy, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b823df59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer: 0, train accuracy: 0.8647427110732412, test accuracy: 0.8469703670942061\n",
      "layer: 1, train accuracy: 0.8850072240951358, test accuracy: 0.8759398496240601\n",
      "layer: 2, train accuracy: 0.9243878042455451, test accuracy: 0.9296771340114993\n",
      "layer: 3, train accuracy: 0.9296113807283369, test accuracy: 0.9283502874834144\n",
      "layer: 4, train accuracy: 0.9333901381839736, test accuracy: 0.930561698363556\n",
      "layer: 5, train accuracy: 0.9325751120660912, test accuracy: 0.9301194161875277\n",
      "layer: 6, train accuracy: 0.9322416922905938, test accuracy: 0.9296771340114993\n",
      "layer: 7, train accuracy: 0.931648946023043, test accuracy: 0.9256965944272446\n",
      "layer: 8, train accuracy: 0.9244989441707109, test accuracy: 0.9201680672268907\n",
      "layer: 9, train accuracy: 0.9147927240395658, test accuracy: 0.9117647058823529\n",
      "layer: 10, train accuracy: 0.9038269180898751, test accuracy: 0.9035824856258293\n",
      "layer: 11, train accuracy: 0.8934538584077354, test accuracy: 0.8971693940734189\n",
      "layer: 12, train accuracy: 0.8651502241321825, test accuracy: 0.8737284387439186\n"
     ]
    }
   ],
   "source": [
    "num_layers = len(train_representations_all)\n",
    "train_accs, test_accs = [], []\n",
    "for l in range(num_layers):\n",
    "    # build new classifier for every layer experiment\n",
    "    classifier, criterion, optimizer = build_classifier(emb_dim, num_labels, device)\n",
    "    # get layer representation \n",
    "    train_representations = train_representations_all[l]\n",
    "    test_representations = test_representations_all[l]\n",
    "    \n",
    "    # train\n",
    "    train_loss, train_accuracy = train(2, train_representations, train_labels_all, \n",
    "          model, tokenizer, sep, model_name, device, \n",
    "          classifier, criterion, optimizer)\n",
    "    train_accs.append(train_accuracy)\n",
    "    # test\n",
    "    test_loss, test_accuracy = evaluate(test_representations, test_labels_all, \n",
    "         model, tokenizer, sep, model_name, device, \n",
    "         classifier, criterion)\n",
    "    test_accs.append(test_accuracy)\n",
    "    print(\"layer: {}, train accuracy: {}, test accuracy: {}\".format(l, train_accuracy, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "96aef545",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "vocabulary = set(\n",
    "    word\n",
    "      for sentence in (train_sentences + test_sentences)\n",
    "      for word in sentence\n",
    ")\n",
    "# all_labels = sum((x.tolist() for x in train_labels), [])\n",
    "all_labels = train_labels_all.tolist()\n",
    "control_map = {word: random.choice(all_labels) for word in vocabulary}\n",
    "\n",
    "control_train_labels = [torch.tensor([control_map[word] for word in sentence]) for sentence in train_sentences]\n",
    "control_test_labels = [torch.tensor([control_map[word] for word in sentence]) for sentence in test_sentences]\n",
    "control_train_labels = torch.tensor(np.concatenate(control_train_labels, 0)).to(device)\n",
    "control_test_labels = torch.tensor(np.concatenate(control_test_labels, 0)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9be5c843",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer: 0, train accuracy: 0.7168154706775831, test accuracy: 0.7293233082706767\n",
      "layer: 1, train accuracy: 0.7085911162153151, test accuracy: 0.7204776647501105\n",
      "layer: 2, train accuracy: 0.6924758270662764, test accuracy: 0.7041132242370632\n",
      "layer: 3, train accuracy: 0.675730745007965, test accuracy: 0.6943830163644406\n",
      "layer: 4, train accuracy: 0.6617271144370763, test accuracy: 0.6786819991154357\n",
      "layer: 5, train accuracy: 0.6435742599933316, test accuracy: 0.66187527642636\n",
      "layer: 6, train accuracy: 0.626940317860186, test accuracy: 0.6410880141530296\n",
      "layer: 7, train accuracy: 0.6030452339495425, test accuracy: 0.6076957098628926\n",
      "layer: 8, train accuracy: 0.5792612899640648, test accuracy: 0.5888987173816895\n",
      "layer: 9, train accuracy: 0.5579965176156781, test accuracy: 0.5667846085802742\n",
      "layer: 10, train accuracy: 0.5349905531063609, test accuracy: 0.5486510393631137\n",
      "layer: 11, train accuracy: 0.5252472863334938, test accuracy: 0.5367094206103494\n",
      "layer: 12, train accuracy: 0.49623976586522434, test accuracy: 0.5064130915524104\n"
     ]
    }
   ],
   "source": [
    "num_layers = len(train_representations_all)\n",
    "control_train_accs, control_test_accs = [], [] \n",
    "for l in range(num_layers):\n",
    "    classifier, criterion, optimizer = build_classifier(emb_dim, num_labels, device)\n",
    "    # get layer representation \n",
    "    train_representations = train_representations_all[l]\n",
    "    test_representations = test_representations_all[l]\n",
    "    \n",
    "    # train\n",
    "    train_loss, train_accuracy = train(2, train_representations, control_train_labels, \n",
    "          model, tokenizer, sep, model_name, device, \n",
    "          classifier, criterion, optimizer)\n",
    "    control_train_accs.append(train_accuracy)\n",
    "    # test\n",
    "    test_loss, test_accuracy = evaluate(test_representations, control_test_labels, \n",
    "         model, tokenizer, sep, model_name, device, \n",
    "         classifier, criterion)\n",
    "    control_test_accs.append(test_accuracy)    \n",
    "    print(\"layer: {}, train accuracy: {}, test accuracy: {}\".format(l, train_accuracy, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c657e231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer: 0, test selectivity: 0.11764705882352944\n",
      "layer: 1, test selectivity: 0.15546218487394958\n",
      "layer: 2, test selectivity: 0.22556390977443608\n",
      "layer: 3, test selectivity: 0.23396727111897386\n",
      "layer: 4, test selectivity: 0.25187969924812026\n",
      "layer: 5, test selectivity: 0.2682441397611677\n",
      "layer: 6, test selectivity: 0.2885891198584697\n",
      "layer: 7, test selectivity: 0.31800088456435205\n",
      "layer: 8, test selectivity: 0.3312693498452012\n",
      "layer: 9, test selectivity: 0.3449800973020787\n",
      "layer: 10, test selectivity: 0.35493144626271567\n",
      "layer: 11, test selectivity: 0.36045997346306946\n",
      "layer: 12, test selectivity: 0.36731534719150816\n"
     ]
    }
   ],
   "source": [
    "for l in range(num_layers):\n",
    "    print(\"layer: {}, test selectivity: {}\".format(l, test_accs[l] - control_test_accs[l]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f966c8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
